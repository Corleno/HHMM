%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Rui Meng. at 2019-02-01 22:07:17 -0800 

%% Saved with string encoding Unicode (UTF-8) 

@ARTICLE{Dyer_2015,
	author = {{Dyer}, Chris and {Ballesteros}, Miguel and {Ling}, Wang and
	{Matthews}, Austin and {Smith}, Noah A.},
	title = "{Transition-Based Dependency Parsing with Stack Long Short-Term Memory}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	year = "2015",
	month = "May",
	eid = {arXiv:1505.08075},
	pages = {arXiv:1505.08075},
	archivePrefix = {arXiv},
	eprint = {1505.08075},
	primaryClass = {cs.CL},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150508075D},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{Arturs_2016,
	author = {{Backurs}, Arturs and {Tzamos}, Christos},
	title = "{Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms},
	year = "2016",
	month = "Jul",
	eid = {arXiv:1607.04229},
	pages = {arXiv:1607.04229},
	archivePrefix = {arXiv},
	eprint = {1607.04229},
	primaryClass = {cs.CC},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160704229B},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{KHREICH_2010,
	title = "On the memory complexity of the forward–backward algorithm",
	journal = "Pattern Recognition Letters",
	volume = "31",
	number = "2",
	pages = "91 - 99",
	year = "2010",
	issn = "0167-8655",
	doi = "https://doi.org/10.1016/j.patrec.2009.09.023",
	url = "http://www.sciencedirect.com/science/article/pii/S0167865509002578",
	author = "Wael Khreich and Eric Granger and Ali Miri and Robert Sabourin",
	keywords = "Hidden Markov Models, Forward–backward, Baum–Welch, Forward Filtering Backward Smoothing, Complexity analysis",
	abstract = "The Forward–backward (FB) algorithm forms the basis for estimation of Hidden Markov Model (HMM) parameters using the Baum–Welch technique. It is however, known to be prohibitively costly when estimation is performed from long observation sequences. Several alternatives have been proposed in literature to reduce the memory complexity of FB at the expense of increased time complexity. In this paper, a novel variation of the FB algorithm – called the Efficient Forward Filtering Backward Smoothing (EFFBS) – is proposed to reduce the memory complexity without the computational overhead. Given an HMM with N states and an observation sequence of length T, both FB and EFFBS algorithms have the same time complexity, O(N2T). Nevertheless, FB has a memory complexity of O(NT), while EFFBS has a memory complexity that is independent of T, O(N). EFFBS requires fewer resources than FB, yet provides the same results."
}

@ARTICLE{Chung_2014,
	author = {{Chung}, Junyoung and {Gulcehre}, Caglar and {Cho}, KyungHyun and
	{Bengio}, Yoshua},
	title = "{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	year = "2014",
	month = "Dec",
	eid = {arXiv:1412.3555},
	pages = {arXiv:1412.3555},
	archivePrefix = {arXiv},
	eprint = {1412.3555},
	primaryClass = {cs.NE},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.3555C},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{Zhang_2016,
	author = {{Zhang}, Ke and {Chao}, Wei-Lun and {Sha}, Fei and {Grauman}, Kristen},
	title = "{Video Summarization with Long Short-term Memory}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	year = "2016",
	month = "May",
	eid = {arXiv:1605.08110},
	pages = {arXiv:1605.08110},
	archivePrefix = {arXiv},
	eprint = {1605.08110},
	primaryClass = {cs.CV},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160508110Z},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{Stephen_2017,
	author = {{Merity}, Stephen and {Shirish Keskar}, Nitish and {Socher}, Richard},
	title = "{Regularizing and Optimizing LSTM Language Models}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	year = "2017",
	month = "Aug",
	eid = {arXiv:1708.02182},
	pages = {arXiv:1708.02182},
	archivePrefix = {arXiv},
	eprint = {1708.02182},
	primaryClass = {cs.CL},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170802182M},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@INPROCEEDINGS{Doestsch_2014,
	author={P. {Doetsch} and M. {Kozielski} and H. {Ney}},
	booktitle={2014 14th International Conference on Frontiers in Handwriting Recognition},
	title={Fast and Robust Training of Recurrent Neural Networks for Offline Handwriting Recognition},
	year={2014},
	volume={},
	number={},
	pages={279-284},
	keywords={handwriting recognition;learning (artificial intelligence);recurrent neural nets;recurrent neural networks training;offline handwriting recognition;modified topology;long short-term memory recurrent neural networks;squashing functions;gating units;mini-batch training;sequence level;sequence chunking approach;English handwriting;French handwriting;GPU based implementation;Logic gates;Training;Hidden Markov models;Databases;Recurrent neural networks;Handwriting recognition;Graphics processing units;handwriting recognition;recurrent neural networks;GPU;batch-training},
	doi={10.1109/ICFHR.2014.54},
	ISSN={2167-6445},
	month={Sep.},}

@ARTICLE{Cho_2014,
	author = {{Cho}, Kyunghyun and {van Merrienboer}, Bart and {Gulcehre}, Caglar and
	{Bahdanau}, Dzmitry and {Bougares}, Fethi and {Schwenk}, Holger and
	{Bengio}, Yoshua},
	title = "{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	year = "2014",
	month = "Jun",
	eid = {arXiv:1406.1078},
	pages = {arXiv:1406.1078},
	archivePrefix = {arXiv},
	eprint = {1406.1078},
	primaryClass = {cs.CL},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1406.1078C},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Hochreiter_1997,
	title = {Long Short-term Memory},
	author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
	year = {1997},
	journal = {Neural Computation},
	page = {1735 - 1780},
}

@Book{Cox_1965,
	title = {The Theory of Stochastic Processes},
	author = {Cox, D. R. and Miller, H. D.},
	year = {1965},
	journal = {London: Chapman and Hall},
}

@article{Bartolonemo_2011,
	title = {Progression of liver cirrhosis to HCC: an application of hidden markov model},
	author = {Bartolomeo, N, and Trerotoli, P. and Serio, G.},
	year = {2011},
	journal = {BMC Med Research Method},
	volume = {11},
}

@article{Liu_2013,
	title = {Longitudinal modeling of glaucoma progression us-ing 2-dimensional continuous-time hidden markov model},
	author = {Liu, Yu-Ying and Ishikawa, Hiroshi and Chen, Mei and Wollstein, Gadi and Schuman Joel S. and Rehg, James M.},
	year = {2013},
	journal = {Med Image Comput Assist Interv.},
}

@inproceedings{Wang_2014,
	author = {Wang, Xiang and Sontag, David and Wang, Fei},
	title = {Unsupervised Learning of Disease Progression Models},
	booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	series = {KDD '14},
	year = {2014},
	isbn = {978-1-4503-2956-9},
	location = {New York, New York, USA},
	pages = {85--94},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2623330.2623754},
	doi = {10.1145/2623330.2623754},
	acmid = {2623754},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {bayesian network, disease progression modeling, markov jump process, medical informatics},
} 



@incollection{Cem_2014,
	title = {Spectral Learning of Mixture of Hidden Markov Models},
	author = {Subakan, Cem and Traa, Johannes and Smaragdis, Paris},
	booktitle = {Advances in Neural Information Processing Systems 27},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	pages = {2249--2257},
	year = {2014},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5518-spectral-learning-of-mixture-of-hidden-markov-models.pdf}
}


@incollection{Gael_2008,
	title = {The Infinite Factorial Hidden Markov Model},
	author = {Jurgen V. Gael and Yee W. Teh and Ghahramani, Zoubin},
	booktitle = {Advances in Neural Information Processing Systems 21},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	pages = {1697--1704},
	year = {2009},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/3518-the-infinite-factorial-hidden-markov-model.pdf}
}

@InProceedings{Kuang_2016,
	author = {Kuang, Zhaobin and Thomson, James and Caldwell, Michael and Peissig, Peggy and Stewart, Ron and Page, David},
	title = {Baseline Regularization for Computational Drug Repositioning with Longitudinal Observational Data},
	booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
	series = {IJCAI'16},
	year = {2016},
	isbn = {978-1-57735-770-4},
	location = {New York, New York, USA},
	pages = {2521--2528},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=3060832.3060974},
	acmid = {3060974},
	publisher = {AAAI Press},
} 

@article{Kuang_2017,
	author = {Kuang, Zhaobin and Peissig, Peggy and Costa, Vitor Santos and Maclin, Richard and Page, David},
    title = {Pharmacovigilance via Baseline Regularization with Large-Scale Longitudinal Observational Data},
    journal = {KDD : proceedings. International Conference on Knowledge Discovery \& Data Mining},
    year = {2017},
    page = {1537-1546},
    doi={10.1145/3097983.3097998},
}

@InProceedings{Bao_2017,
	title = 	 {Hawkes Process Modeling of Adverse Drug Reactions with Longitudinal Observational Data},
	author = 	 {Yujia Bao and Zhaobin Kuang and Peggy Peissig and David Page and Rebecca Willett},
	booktitle = 	 {Proceedings of the 2nd Machine Learning for Healthcare Conference},
	pages = 	 {177--190},
	year = 	 {2017},
	editor = 	 {Finale Doshi-Velez and Jim Fackler and David Kale and Rajesh Ranganath and Byron Wallace and Jenna Wiens},
	volume = 	 {68},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Boston, Massachusetts},
	month = 	 {18--19 Aug},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v68/bao17a/bao17a.pdf},
	url = 	 {http://proceedings.mlr.press/v68/bao17a.html},
	abstract = 	 {Adverse drug reaction (ADR) discovery is the task of identifying unexpected and negative events caused by pharmaceutical products. This paper describes a log-linear Hawkes process model for ADR discovery from longitudinal observational data such as electronic health records (EHRs). The proposed method leverages the irregular time-stamped events in EHRs to represent the time-varying effect of various drugs on the occurrence rate of adverse events. Experimental results on a large-scale cohort of real-world EHRs demonstrate that the proposed method outperforms a leading approach, multiple self-controlled case series (Simpson et al., 2013), in identifying benchmark ADRs defined by the Observational Medical Outcomes Partnership.}
}


@article{Simpson_2013,
	author = {Simpson, Shawn E. and Madigan, David and Zorych, Ivan and Schuemie, Martijn J. and Ryan, Patrick B. and Suchard, Marc A.},
	title = {Multiple Self-Controlled Case Series for Large-Scale Longitudinal Observational Databases},
	journal = {Biometrics},
	volume = {69},
	number = {4},
	pages = {893-902},
	keywords = {Big Data, Conditional Poisson regression, Cyclic coordinate descent, Drug safety, Postmarketing surveillance, Regularized regression, Self-controlled case series, Statistical computing},
	doi = {10.1111/biom.12078},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12078},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12078},
	abstract = {Summary Characterization of relationships between time-varying drug exposures and adverse events (AEs) related to health outcomes represents the primary objective in postmarketing drug safety surveillance. Such surveillance increasingly utilizes large-scale longitudinal observational databases (LODs), containing time-stamped patient-level medical information including periods of drug exposure and dates of diagnoses for millions of patients. Statistical methods for LODs must confront computational challenges related to the scale of the data, and must also address confounding and other biases that can undermine efforts to estimate effect sizes. Methods that compare on-drug with off-drug periods within patient offer specific advantages over between patient analysis on both counts. To accomplish these aims, we extend the self-controlled case series (SCCS) for LODs. SCCS implicitly controls for fixed multiplicative baseline covariates since each individual acts as their own control. In addition, only exposed cases are required for the analysis, which is computationally advantageous. The standard SCCS approach is usually used to assess single drugs and therefore estimates marginal associations between individual drugs and particular AEs. Such analyses ignore confounding drugs and interactions and have the potential to give misleading results. In order to avoid these difficulties, we propose a regularized multiple SCCS approach that incorporates potentially thousands or more of time-varying confounders such as other drugs. The approach successfully handles the high dimensionality and can provide a sparse solution via an regularizer. We present details of the model and the associated optimization procedure, as well as results of empirical investigations.},
	year = {2013},
}

@ARTICLE{Kanungo_2002,
	author={T. Kanungo and D. M. Mount and N. S. Netanyahu and C. D. Piatko and R. Silverman and A. Y. Wu},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={An efficient k-means clustering algorithm: analysis and implementation},
	year={2002},
	volume={24},
	number={7},
	pages={881-892},
	keywords={pattern clustering;filtering theory;covariance matrices;k-means clustering algorithm;mean squared distance;Lloyd algorithm;filtering algorithm;kd-tree;data structure;data-sensitive analysis;color quantization;data compression;image segmentation;Clustering algorithms;Algorithm design and analysis;Iterative algorithms;Filtering algorithms;Machine learning algorithms;Data compression;Pattern recognition;Data mining;Data structures;Data analysis},
	doi={10.1109/TPAMI.2002.1017616},
	ISSN={0162-8828},
	month={July},}

@INPROCEEDINGS{Kearns_2013,
	author = {Kearns, Michael and Mansour, Yishay and Ng, Andrew Y. },
	title = {An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering},
	booktitle = {UAI},
	year = {1997},
	pages = {282--293},
	publisher = {Morgan Kaufmann}
}


@article{titman_general_2008,
	title = {A general goodness-of-fit test for {Markov} and hidden {Markov} models},
	volume = {27},
	issn = {02776715, 10970258},
	url = {http://doi.wiley.com/10.1002/sim.3033},
	doi = {10.1002/sim.3033},
	language = {en},
	number = {12},
	urldate = {2017-10-26},
	journal = {Statistics in Medicine},
	author = {Titman, Andrew C. and Sharples, Linda D.},
	month = may,
	year = {2008},
	pages = {2177--2195},
	file = {Titman and Sharples - 2008 - A general goodness-of-fit test for Markov and hidd.pdf:/Users/soper3/Zotero/storage/FPP34JXP/Titman and Sharples - 2008 - A general goodness-of-fit test for Markov and hidd.pdf:application/pdf}
}


@article{Liu_1989,
	author = {Liu, Dong C. and Nocedal, Jorge},
	title = {On the Limited Memory BFGS Method for Large Scale Optimization},
	journal = {Math. Program.},
	issue_date = {August    1989},
	volume = {45},
	number = {1-3},
	month = aug,
	year = {1989},
	issn = {0025-5610},
	pages = {503--528},
	numpages = {26},
	url = {https://doi.org/10.1007/BF01589116},
	doi = {10.1007/BF01589116},
	acmid = {3112866},
	publisher = {Springer-Verlag New York, Inc.},
	address = {Secaucus, NJ, USA},
	keywords = {Large scale nonlinear optimization, conjugate gradient method, limited memory methods, partitioned quasi-Newton method},
} 

@misc{Soper_et_al_2018,
	Author = {Soper, Braden and Meng, Rui and Nygard, Jan F. and Nygard, Mari and Lee, Herbert},
	Date-Added = {2019-02-01 22:00:03 -0800},
	Date-Modified = {2019-02-01 22:07:13 -0800},
	Howpublished = {Abstract from HPC Applications in Precision Medicine, Frankfurt, Germany},
	Month = {June},
	Title = {An HPC Application to Population-Level Cancer Screening Data},
	Year = {2018}}

@article{Salimbeni_2017,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/\#abs/2017arXiv170508933S},
	Archiveprefix = {arXiv},
	Author = {{Salimbeni}, Hugh and {Deisenroth}, Marc},
	Eid = {arXiv:1705.08933},
	Eprint = {1705.08933},
	Journal = {arXiv e-prints},
	Keywords = {Statistics - Machine Learning},
	Month = May,
	Pages = {arXiv:1705.08933},
	Primaryclass = {stat.ML},
	Title = {{Doubly Stochastic Variational Inference for Deep Gaussian Processes}},
	Year = 2017}

@article{Kaiser_2018,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv181007158K},
	Archiveprefix = {arXiv},
	Author = {{Kaiser}, Markus and {Otte}, Clemens and {Runkler}, Thomas and {Ek}, Carl Henrik},
	Eid = {arXiv:1810.07158},
	Eprint = {1810.07158},
	Journal = {arXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = Oct,
	Pages = {arXiv:1810.07158},
	Primaryclass = {stat.ML},
	Title = {{Multimodal Deep Gaussian Processes}},
	Year = 2018}

@article{Kumar_2018,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/\#abs/2018arXiv180601655K},
	Archiveprefix = {arXiv},
	Author = {{Kumar}, Vinayak and {Singh}, Vaibhav and {Srijith}, P.~K. and {Damianou}, Andreas},
	Eid = {arXiv:1806.01655},
	Eprint = {1806.01655},
	Journal = {arXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	Month = Jun,
	Pages = {arXiv:1806.01655},
	Primaryclass = {stat.ML},
	Title = {{Deep Gaussian Processes with Convolutional Kernels}},
	Year = 2018}

@article{Damianou_2013,
	Author = {Damianou, Andreas C. and Lawrence, Niel D.},
	Journal = {International Conference on Artificial Intelligence and Statistics},
	Title = {Deep Gaussian Processes},
	Year = {2013}}

@article{Braden_2019,
	Author = {Soper, Braden and Nygard Mari and Abdulla, Ghaled and Meng, Rui and Nygard, Jan F.},
	Date-Modified = {2019-02-01 21:59:57 -0800},
	Journal = {Biometrics},
	Title = {From Static Guidelines to Personalized Screening: Applying Hidden Markov Models to Cervical Cancer Screening},
	Year = {2019}}

@article{Kirby_1994,
	Author = {Kirby, A. J. and Spiegelhalter, D. J.},
	Journal = {Case Studies in Biometry},
	Title = {Modeling the Precursors of Cervical Cancer},
	Year = {1994}}

@article{Sonnernberg_1993,
	Author = {Sonnenberg, Frank A. and Beck J. Robert},
	Journal = {Med Decis Making},
	Pages = {322-338},
	Title = {Markov Models in Medical Decision Making: A Practical Guide},
	Volume = {13},
	Year = {1993}}

@article{Canfell_2004,
	Author = {Canfell, K and Barnabas, R and Patnick J and Beral V},
	Journal = {British journal of cancer},
	Number = {3},
	Pages = {530-536},
	Title = {The predicted effect of changes in cervical screening practice in the UK: results from a modelling study},
	Volume = {91},
	Year = {2004}}

@article{Myers_2000,
	Abstract = {{The authors constructed a Markov model as part of a systematic review of cervical cytology conducted at the Duke University Evidence-based Practice Center (Durham, North Carolina) between October 1997 and September 1998. The model incorporated states for human papillomavirus infection (HPV), low- and high-grade squamous intraepithelial lesions, and cervical cancer stages I--IV to simulate the natural history of HPV infection in a cohort of women from ages 15 to 85 years. The age-specific incidence rate of HPV, and regression and progression rates of HPV and squamous intraepithelial lesions, were obtained from the literature. The effects of varying natural history parameters on cervical cancer incidence were evaluated by using sensitivity analysis. The base-case model resulted in a lifetime cervical cancer risk of 3.67\\% and a lifetime cervical cancer mortality risk of 1.26\\%, with a peak incidence of 81/100, 000 at age 50 years. Age-specific distributions of precursors were similar to reported data. Lifetime risk of cancer was most sensitive to the incidence of HPV and the probability of rapid HPV progression to high-grade lesions (two- to threefold variations in risk). The model approximates the age-specific incidence of cervical cancer and provides a tool for evaluating the natural history of HPV infection and cervicai cancer carcinogenesis as well as the effectiveness and cost-effectiveness of primary and secondary prevention strategies. Am J Epidemiol 2000; 151: 1158-71.}},
	Author = {Myers, Evan R. and McCrory, Douglas C. and Nanda, Kavita and Bastian, Lori and Matchar, David B.},
	Doi = {10.1093/oxfordjournals.aje.a010166},
	Eprint = {http://oup.prod.sis.lan/aje/article-pdf/151/12/1158/119536/151-12-1158.pdf},
	Issn = {0002-9262},
	Journal = {American Journal of Epidemiology},
	Month = {06},
	Number = {12},
	Pages = {1158-1171},
	Title = {{Mathematical Model for the Natural History of Human Papillomavirus Infection and Cervical Carcinogenesis}},
	Url = {https://dx.doi.org/10.1093/oxfordjournals.aje.a010166},
	Volume = {151},
	Year = {2000},
	Bdsk-Url-1 = {https://dx.doi.org/10.1093/oxfordjournals.aje.a010166}}

@inbook{MacKay1996,
	Abstract = {Bayesian probability theory provides a unifying framework for data modeling. In this framework, the overall aims are to find models that are well matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers, and weight decay constants) also then can be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This provides powerful and practical methods for controlling, comparing, and using adaptive network models. This chapter describes numerical techniques based on Gaussian approximations for implementation of these methods.},
	Address = {New York, NY},
	Author = {MacKay, David J. C.},
	Booktitle = {Models of Neural Networks III: Association, Generalization, and Representation},
	Doi = {10.1007/978-1-4612-0723-8_6},
	Isbn = {978-1-4612-0723-8},
	Pages = {211--254},
	Publisher = {Springer New York},
	Title = {Bayesian Methods for Backpropagation Networks},
	Url = {https://doi.org/10.1007/978-1-4612-0723-8_6},
	Year = {1996},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4612-0723-8_6}}

@book{Rasmussen_2005,
	Author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	Isbn = {026218253X},
	Publisher = {The MIT Press},
	Title = {Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)},
	Year = {2005}}

@article{Seeger_2003,
	Acmid = {944929},
	Author = {Seeger, Matthias},
	Doi = {10.1162/153244303765208386},
	Issn = {1532-4435},
	Issue_Date = {3/1/2003},
	Journal = {J. Mach. Learn. Res.},
	Keywords = {Bayesian learning, Gaussian processes, Gibbs classifier, Kernel machines, PAC-Bayesian framework, convex duality, generalisation error bounds, sparse approximations},
	Month = mar,
	Numpages = {37},
	Pages = {233--269},
	Publisher = {JMLR.org},
	Title = {Pac-bayesian Generalisation Error Bounds for Gaussian Process Classification},
	Url = {https://doi.org/10.1162/153244303765208386},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1162/153244303765208386}}

@article{Finley_2009,
	Author = {Finley, Andrew Sang and Huiyan Banerjee, Sudipto and Gelfand, Alan},
	Journal = {Computational statistics and data analysis},
	Title = {Improving the performance of predictive process modeling for large datasets},
	Year = {2009}}

@article{Hensman_2012,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1206.5162H},
	Archiveprefix = {arXiv},
	Author = {{Hensman}, J. and {Rattray}, M. and {Lawrence}, N.~D.},
	Eprint = {1206.5162},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = jun,
	Title = {{Fast Variational Inference in the Conjugate Exponential Family}},
	Year = 2012}

@inproceedings{Lawrence_2003,
	Author = {Neil D. Lawrence},
	Booktitle = {NIPS},
	Title = {Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data},
	Year = {2003}}

@inproceedings{Lawrence_2007,
	Author = {Carl Henrik Ek and Philip H. S. Torr and Neil D. Lawrence},
	Booktitle = {MLMI},
	Title = {Gaussian Process Latent Variable Models for Human Pose Estimation},
	Year = {2007}}

@inproceedings{Lawrence_2007_HGP,
	Acmid = {1273557},
	Address = {New York, NY, USA},
	Author = {Lawrence, Neil D. and Moore, Andrew J.},
	Booktitle = {Proceedings of the 24th International Conference on Machine Learning},
	Doi = {10.1145/1273496.1273557},
	Isbn = {978-1-59593-793-3},
	Location = {Corvalis, Oregon, USA},
	Numpages = {8},
	Pages = {481--488},
	Publisher = {ACM},
	Series = {ICML '07},
	Title = {Hierarchical Gaussian Process Latent Variable Models},
	Url = {http://doi.acm.org/10.1145/1273496.1273557},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1273496.1273557},
	Bdsk-Url-2 = {https://doi.org/10.1145/1273496.1273557}}

@inproceedings{Lawrence_2006,
	Acmid = {1143909},
	Address = {New York, NY, USA},
	Author = {Lawrence, Neil D. and Qui\~{n}onero-Candela, Joaquin},
	Booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	Doi = {10.1145/1143844.1143909},
	Isbn = {1-59593-383-2},
	Location = {Pittsburgh, Pennsylvania, USA},
	Numpages = {8},
	Pages = {513--520},
	Publisher = {ACM},
	Series = {ICML '06},
	Title = {Local Distance Preservation in the GP-LVM Through Back Constraints},
	Url = {http://doi.acm.org/10.1145/1143844.1143909},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1143844.1143909},
	Bdsk-Url-2 = {https://doi.org/10.1145/1143844.1143909}}

@inproceedings{Titsias_2009,
	Abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs  are defined to be variational parameters  which are selected by minimizing  the Kullback-Leibler divergence between  the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	Address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
	Author = {Michalis Titsias},
	Booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
	Editor = {David van Dyk and Max Welling},
	Month = {16--18 Apr},
	Pages = {567--574},
	Pdf = {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Variational Learning of Inducing Variables in Sparse Gaussian Processes},
	Url = {http://proceedings.mlr.press/v5/titsias09a.html},
	Volume = {5},
	Year = {2009},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v5/titsias09a.html}}

@inproceedings{Titsias_2010,
	Abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
	Address = {Chia Laguna Resort, Sardinia, Italy},
	Author = {Michalis Titsias and Neil D. Lawrence},
	Booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	Editor = {Yee Whye Teh and Mike Titterington},
	Month = {13--15 May},
	Pages = {844--851},
	Pdf = {http://proceedings.mlr.press/v9/titsias10a/titsias10a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Bayesian Gaussian Process Latent Variable Model},
	Url = {http://proceedings.mlr.press/v9/titsias10a.html},
	Volume = {9},
	Year = {2010},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v9/titsias10a.html}}

@article{Gal_2015,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150302182G},
	Archiveprefix = {arXiv},
	Author = {{Gal}, Y. and {Chen}, Y. and {Ghahramani}, Z.},
	Eprint = {1503.02182},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning},
	Month = mar,
	Primaryclass = {stat.ML},
	Title = {{Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data}},
	Year = 2015}

@inproceedings{Khan_2012,
	Abstract = {The development of accurate models and efficient algorithms for the analysis of multivariate categorical data are important and long-standing problems in machine learning and computational statistics. In this paper, we focus on modeling categorical data using Latent Gaussian Models (LGMs). We propose a novel stick-breaking likelihood function for categorical LGMs that exploits accurate linear and quadratic bounds on the logistic log-partition function, leading to an effective variational inference and learning framework. We thoroughly compare our approach to existing algorithms for multinomial logit/probit likelihoods on several problems, including inference in multinomial Gaussian process classification and learning in latent factor models. Our extensive comparisons demonstrate that our stick-breaking model effectively captures correlation in discrete data and is well suited for the analysis of categorical data.},
	Address = {La Palma, Canary Islands},
	Author = {Mohammad Khan and Shakir Mohamed and Benjamin Marlin and Kevin Murphy},
	Booktitle = {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
	Editor = {Neil D. Lawrence and Mark Girolami},
	Month = {21--23 Apr},
	Pages = {610--618},
	Pdf = {http://proceedings.mlr.press/v22/khan12/khan12.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {A Stick-Breaking Likelihood for Categorical Data Analysis with Latent Gaussian Models},
	Url = {http://proceedings.mlr.press/v22/khan12.html},
	Volume = {22},
	Year = {2012},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v22/khan12.html}}

@article{Kingma_2013,
	Added-At = {2018-08-13T00:00:00.000+0200},
	Author = {Kingma, Diederik P. and Welling, Max},
	Biburl = {https://www.bibsonomy.org/bibtex/2486ad13a443259d137cb57be1dc77002/dblp},
	Ee = {http://arxiv.org/abs/1312.6114},
	Interhash = {85731e0fbdb10b8543ea9f55301b37a5},
	Intrahash = {486ad13a443259d137cb57be1dc77002},
	Journal = {CoRR},
	Keywords = {dblp},
	Timestamp = {2018-08-14T14:45:20.000+0200},
	Title = {Auto-Encoding Variational Bayes.},
	Url = {http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13},
	Volume = {abs/1312.6114},
	Year = 2013,
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/journals/corr/corr1312.html#KingmaW13}}

@techreport{Duchi_2011,
	Abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods significantly outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	Institution = {EECS Department, University of California, Berkeley},
	Month = {Mar},
	Number = {UCB/EECS-2010-24},
	Title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	Url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-24.html},
	Year = {2010},
	Bdsk-Url-1 = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-24.html}}

@article{Zeiler_2012,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1212.5701Z},
	Archiveprefix = {arXiv},
	Author = {{Zeiler}, M.~D.},
	Eprint = {1212.5701},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Machine Learning},
	Month = dec,
	Title = {{ADADELTA: An Adaptive Learning Rate Method}},
	Year = 2012}

@article{Hilton_2012,
	Absurl = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	Author = {Hinton, Geoff},
	Journal = {Lecture in his Coursera},
	Title = {Lecture 6e of his Coursera Class},
	Year = {2012}}

@article{Kingma_2014,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P. and {Ba}, J.},
	Eprint = {1412.6980},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Machine Learning},
	Month = dec,
	Title = {{Adam: A Method for Stochastic Optimization}},
	Year = 2014}

@article{Shen_2018,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180202032S},
	Archiveprefix = {arXiv},
	Author = {{Shen}, X. and {Su}, H. and {Niu}, S. and {Demberg}, V.},
	Eprint = {1802.02032},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	Month = feb,
	Primaryclass = {cs.CL},
	Title = {{Improving Variational Encoder-Decoders in Dialogue Generation}},
	Year = 2018}

@article{Bowman_2015,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106349B},
	Archiveprefix = {arXiv},
	Author = {{Bowman}, S.~R. and {Vilnis}, L. and {Vinyals}, O. and {Dai}, A.~M. and {Jozefowicz}, R. and {Bengio}, S.},
	Eprint = {1511.06349},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	Month = nov,
	Title = {{Generating Sentences from a Continuous Space}},
	Year = 2015}

@article{Berchtold_2002,
	Author = {Berchtold, Andr{\'e} and Raftery, Adrian},
	Doi = {10.1214/ss/1042727943},
	Fjournal = {Statistical Science},
	Journal = {Statist. Sci.},
	Month = {08},
	Number = {3},
	Pages = {328--356},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {The Mixture Transition Distribution Model for High-Order Markov Chains and Non-Gaussian Time Series},
	Url = {https://doi.org/10.1214/ss/1042727943},
	Volume = {17},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1214/ss/1042727943}}

@article{Tank_2017,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170602781T},
	Archiveprefix = {arXiv},
	Author = {{Tank}, A. and {Fox}, E.~B. and {Shojaie}, A.},
	Eprint = {1706.02781},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Methodology},
	Month = jun,
	Primaryclass = {stat.ME},
	Title = {{Granger Causality Networks for Categorical Time Series}},
	Year = 2017}

@article{Joao_2014,
	Abstract = {ABSTRACTWe propose a new model for multivariate Markov chains of order one or higher on the basis of the mixture transition distribution (MTD) model. We call it the MTD-Probit. The proposed model presents two attractive features: it is completely free of constraints, thereby facilitating the estimation procedure, and it is more precise at estimating the transition probabilities of a multivariate or higher-order Markov chain than the standard MTD model.},
	Author = {Nicolau, Jo{\~a}o},
	Doi = {10.1111/sjos.12087},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/sjos.12087},
	Journal = {Scandinavian Journal of Statistics},
	Keywords = {high-order Markov chains, maximum likelihood method, mixture transition distribution, multivariate Markov chains},
	Number = {4},
	Pages = {1124-1135},
	Title = {A New Model for Multivariate Markov Chains},
	Url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12087},
	Volume = {41},
	Year = 2014,
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12087},
	Bdsk-Url-2 = {https://doi.org/10.1111/sjos.12087}}

@article{Lazaro_2010,
	Acmid = {1859914},
	Author = {L\'{a}zaro-Gredilla, Miguel and Qui\~{n}onero-Candela, Joaquin and Rasmussen, Carl Edward and Figueiras-Vidal, An\'{\i}bal R.},
	Issn = {1532-4435},
	Issue_Date = {3/1/2010},
	Journal = {J. Mach. Learn. Res.},
	Month = aug,
	Numpages = {17},
	Pages = {1865--1881},
	Publisher = {JMLR.org},
	Title = {Sparse Spectrum Gaussian Process Regression},
	Url = {http://dl.acm.org/citation.cfm?id=1756006.1859914},
	Volume = {11},
	Year = {2010},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1756006.1859914}}

@article{Tipping_1999,
	Abstract = {
	Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based upon a probability model. In this paper we demonstrate how the principal axes of a set of observed data vectors may be determined through maximum-likelihood estimation of parameters in a latent variable model closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.
	},
	Author = {Tipping, M. E. and Bishop, Christopher},
	Journal = {Journal of the Royal Statistical Society, Series B},
	Month = {January},
	Note = {Available from http://www.ncrg.aston.ac.uk/Papers/index.html},
	Number = {3},
	Pages = {611-622},
	Title = {Probabilistic Principal Component Analysis},
	Url = {https://www.microsoft.com/en-us/research/publication/probabilistic-principal-component-analysis/},
	Volume = {21},
	Year = {1999},
	Bdsk-Url-1 = {https://www.microsoft.com/en-us/research/publication/probabilistic-principal-component-analysis/}}

@article{Scholkopf_1998,
	Author = {Sch\"olkopf Bernhard and Smola Alexander and M\"uller Klaus-Robert},
	Journal = {Advances in Kernel Methods - Support Vector Learning},
	Pages = {327-352},
	Publisher = {MIT Press},
	Title = {Kernel principle component analysis},
	Year = {1998}}

@article{Wang_2012,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2012arXiv1207.3538W},
	Archiveprefix = {arXiv},
	Author = {{Wang}, Q.},
	Eprint = {1207.3538},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jul,
	Primaryclass = {cs.CV},
	Title = {{Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models}},
	Year = 2012}

@article{Tresp_2000,
	Abstract = { The Bayesian committee machine (BCM) is a novel approach to combining estimators that were trained on different data sets. Although the BCM can be applied to the combination of any kind of estimators, the main foci are gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data. Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator. The BCM also provides a new solution for on-line learning with potential applications to data mining. We apply the BCM to systems with fixed basis functions and discuss its relationship to gaussian process regression. Finally, we show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input-dependent combination of estimators. },
	Author = {Tresp, Volker},
	Doi = {10.1162/089976600300014908},
	Eprint = {https://doi.org/10.1162/089976600300014908},
	Journal = {Neural Computation},
	Number = {11},
	Pages = {2719-2741},
	Title = {A Bayesian Committee Machine},
	Url = {https://doi.org/10.1162/089976600300014908},
	Volume = {12},
	Year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1162/089976600300014908}}

@incollection{Snelson_2006,
	Author = {Edward Snelson and Ghahramani, Zoubin},
	Booktitle = {Advances in Neural Information Processing Systems 18},
	Editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
	Pages = {1257--1264},
	Publisher = {MIT Press},
	Title = {Sparse Gaussian Processes using Pseudo-inputs},
	Url = {http://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/2857-sparse-gaussian-processes-using-pseudo-inputs.pdf}}

@article{Hensman_2013,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1309.6835H},
	Archiveprefix = {arXiv},
	Author = {{Hensman}, J. and {Fusi}, N. and {Lawrence}, N.~D.},
	Eprint = {1309.6835},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = sep,
	Title = {{Gaussian Processes for Big Data}},
	Year = 2013}

@article{Raj_2011,
	Author = {Guhaniyogi, Rajarshi and Finley, Andrew and Banerjee, Sudipto and Gelfand, Alan},
	Journal = {Environmetrics},
	Title = {Adaptive Gaussian Predictive Process Models for Large Spatial Datasets},
	Year = 2011}

@article{Bush_1996,
	Author = {Bush, Christopher A. and Maceachern, Steven N.},
	Doi = {10.1093/biomet/83.2.275},
	Eprint = {/oup/backfile/content_public/journal/biomet/83/2/10.1093/biomet/83.2.275/2/83-2-275.pdf},
	Journal = {Biometrika},
	Number = {2},
	Pages = {275-285},
	Title = {A semiparametric Bayesian model for randomised block designs},
	Url = {http://dx.doi.org/10.1093/biomet/83.2.275},
	Volume = {83},
	Year = {1996},
	Bdsk-Url-1 = {http://dx.doi.org/10.1093/biomet/83.2.275}}

@article{Connor_1969,
	Abstract = {Concepts of independence for nonnegative continuous random variables, X<sub>1</sub>,⋯, X<sub>k</sub>, subject to the constraint Σ X<sub>i</sub> = 1 are developed. These concepts provide a means of modeling random vectors of proportions which is useful in analyzing certain kinds of data; and which may be of interest in quantifying prior opinions about multinomial parameters. A generalization of the Dirichlet distribution is given, and its relation to the Dirichlet is simply indicated by means of the concepts. The concepts are used to obtain conclusions of biological interest for data on bone composition in rats and scute growth in turtles.},
	Author = {Robert J. Connor and James E. Mosimann},
	Issn = {01621459},
	Journal = {Journal of the American Statistical Association},
	Number = {325},
	Pages = {194--206},
	Publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	Title = {Concepts of Independence for Proportions with a Generalization of the Dirichlet Distribution},
	Url = {http://www.jstor.org/stable/2283728},
	Volume = {64},
	Year = {1969},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2283728}}

@article{Beckmann_2016,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160501019L},
	Archiveprefix = {arXiv},
	Author = {{Llera}, A. and {Beckmann}, C.~F.},
	Eprint = {1605.01019},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Methodology},
	Month = may,
	Primaryclass = {stat.ME},
	Title = {{Estimating an Inverse Gamma distribution}},
	Year = 2016}

@article{Jang_2016,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161101144J},
	Archiveprefix = {arXiv},
	Author = {{Jang}, E. and {Gu}, S. and {Poole}, B.},
	Eprint = {1611.01144},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	Month = nov,
	Primaryclass = {stat.ML},
	Title = {{Categorical Reparameterization with Gumbel-Softmax}},
	Year = 2016}

@article{Jaderberg_2015,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150602025J},
	Archiveprefix = {arXiv},
	Author = {{Jaderberg}, M. and {Simonyan}, K. and {Zisserman}, A. and {Kavukcuoglu}, K.},
	Eprint = {1506.02025},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = jun,
	Primaryclass = {cs.CV},
	Title = {{Spatial Transformer Networks}},
	Year = 2015}

@incollection{Wang_2006,
	Author = {Jack Wang and Hertzmann, Aaron and Fleet, David J},
	Booktitle = {Advances in Neural Information Processing Systems 18},
	Editor = {Y. Weiss and B. Sch\"{o}lkopf and J. C. Platt},
	Pages = {1441--1448},
	Publisher = {MIT Press},
	Title = {Gaussian Process Dynamical Models},
	Url = {http://papers.nips.cc/paper/2783-gaussian-process-dynamical-models.pdf},
	Year = {2006},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/2783-gaussian-process-dynamical-models.pdf}}

@article{Wang_2008,
	Author = {J. M. Wang and D. J. Fleet and A. Hertzmann},
	Doi = {10.1109/TPAMI.2007.1167},
	Issn = {0162-8828},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {Gaussian processes;learning (artificial intelligence);motion estimation;time series;Gaussian process dynamical model;nonlinear time series analysis;learning model;low-dimensional latent space;human motion capture data;Gaussian processes;Humans;Hidden Markov models;Motion analysis;Animation;Training data;Time series analysis;Tracking;Predictive models;Lifting equipment;machine learning;motion;tracking;animation;stochastic processes;time series analysis;machine learning;motion;tracking;animation;stochastic processes;time series analysis;Algorithms;Artificial Intelligence;Biomedical Engineering;Computer Simulation;Gait;Humans;Linear Models;Models, Biological;Movement;Nonlinear Dynamics;Video Recording;Walking},
	Month = {Feb},
	Number = {2},
	Pages = {283-298},
	Title = {Gaussian Process Dynamical Models for Human Motion},
	Volume = {30},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/TPAMI.2007.1167}}

@article{Damianou_2016,
	Author = {Andreas C. Damianou and Michalis K. Titsias and Neil D. Lawrence},
	Journal = {Journal of Machine Learning Research},
	Number = {42},
	Pages = {1-62},
	Title = {Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes},
	Url = {http://jmlr.org/papers/v17/damianou16a.html},
	Volume = {17},
	Year = {2016},
	Bdsk-Url-1 = {http://jmlr.org/papers/v17/damianou16a.html}}

@article{MacKay_1992,
	Author = {D. J. C. MacKay},
	Doi = {10.1162/neco.1992.4.3.415},
	Issn = {0899-7667},
	Journal = {Neural Computation},
	Month = {May},
	Number = {3},
	Pages = {415-447},
	Title = {Bayesian Interpolation},
	Volume = {4},
	Year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1162/neco.1992.4.3.415}}

@book{Neal_1996,
	Address = {Berlin, Heidelberg},
	Author = {Neal, Radford M.},
	Isbn = {0387947248},
	Publisher = {Springer-Verlag},
	Title = {Bayesian Learning for Neural Networks},
	Year = {1996}}

@article{Cressie_1993,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/1993CG.....19..615H},
	Author = {{Haining}, R.},
	Doi = {10.1016/0098-3004(93)90088-M},
	Journal = {Computers and Geosciences},
	Month = apr,
	Pages = {615-616},
	Title = {Statistics for spatial data},
	Volume = 19,
	Year = 1993,
	Bdsk-Url-1 = {https://doi.org/10.1016/0098-3004(93)90088-M}}

@misc{Banerjee_2008,
	Author = {Sudipto Banerjee and Alan E. Gelfand and Andrew O. Finley and Huiyan Sang},
	Title = {Gaussian predictive process models for large spatial data sets},
	Year = {2008}}

@article{Eidsvik_2012,
	Abstract = {The challenges of estimating hierarchical spatial models to large datasets are addressed. With the increasing availability of geocoded scientific data, hierarchical models involving spatial processes have become a popular method for carrying out spatial inference. Such models are customarily estimated using Markov chain Monte Carlo algorithms that, while immensely flexible, can become prohibitively expensive. In particular, fitting hierarchical spatial models often involves expensive decompositions of dense matrices whose computational complexity increases in cubic order with the number of spatial locations. Such matrix computations are required in each iteration of the Markov chain Monte Carlo algorithm, rendering them infeasible for large spatial datasets. The computational challenges in analyzing large spatial datasets are considered by merging two recent developments. First, the predictive process model is used as a reduced-rank spatial process, to diminish the dimensionality of the model. Then a computational framework is developed for estimating predictive process models using the integrated nested Laplace approximation. The settings where the first stage likelihood is Gaussian or non-Gaussian are discussed. Issues such as predictions and model comparisons are also discussed. Results are presented for synthetic data and several environmental datasets.},
	Author = {Jo Eidsvik and Andrew O. Finley and Sudipto Banerjee and H{\aa}vard Rue},
	Doi = {https://doi.org/10.1016/j.csda.2011.10.022},
	Issn = {0167-9473},
	Journal = {Computational Statistics \& Data Analysis},
	Keywords = {Approximate Bayesian inference, Computational statistics, Gaussian processes, Geostatistics, Laplace approximation, Predictive process model},
	Number = {6},
	Pages = {1362 - 1380},
	Title = {Approximate Bayesian inference for large spatial datasets using predictive process models},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167947311003938},
	Volume = {56},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167947311003938},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.csda.2011.10.022}}

@article{Besag_1995,
	Author = {Besag, Julian and Green, Peter and Higdon, David and Mengersen, Kerrie},
	Doi = {10.1214/ss/1177010123},
	Fjournal = {Statistical Science},
	Journal = {Statist. Sci.},
	Month = {02},
	Number = {1},
	Pages = {3--41},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Bayesian Computation and Stochastic Systems},
	Url = {https://doi.org/10.1214/ss/1177010123},
	Volume = {10},
	Year = {1995},
	Bdsk-Url-1 = {https://doi.org/10.1214/ss/1177010123}}

@misc{Qi_2002,
	Author = {Yuan Qi and Thomas P. Minka},
	Title = {Hessian-based Markov Chain Monte-carlo Algorithms},
	Year = {2002}}

@article{freedman1963,
	Author = {Freedman, David A.},
	Doi = {10.1214/aoms/1177703871},
	Fjournal = {The Annals of Mathematical Statistics},
	Journal = {Ann. Math. Statist.},
	Month = {12},
	Number = {4},
	Pages = {1386--1403},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {On the Asymptotic Behavior of Bayes' Estimates in the Discrete Case},
	Url = {https://doi.org/10.1214/aoms/1177703871},
	Volume = {34},
	Year = {1963},
	Bdsk-Url-1 = {https://doi.org/10.1214/aoms/1177703871}}

@article{fabius1964,
	Author = {Fabius, J.},
	Doi = {10.1214/aoms/1177703584},
	Fjournal = {The Annals of Mathematical Statistics},
	Journal = {Ann. Math. Statist.},
	Month = {06},
	Number = {2},
	Pages = {846--856},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Asymptotic Behavior of Bayes' Estimates},
	Url = {https://doi.org/10.1214/aoms/1177703584},
	Volume = {35},
	Year = {1964},
	Bdsk-Url-1 = {https://doi.org/10.1214/aoms/1177703584}}

@article{ferguson1973,
	Author = {Ferguson, Thomas S.},
	Doi = {10.1214/aos/1176342360},
	Fjournal = {The Annals of Statistics},
	Journal = {Ann. Statist.},
	Month = {03},
	Number = {2},
	Pages = {209--230},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {A Bayesian Analysis of Some Nonparametric Problems},
	Url = {https://doi.org/10.1214/aos/1176342360},
	Volume = {1},
	Year = {1973},
	Bdsk-Url-1 = {https://doi.org/10.1214/aos/1176342360}}

@article{walker1999,
	Abstract = {In recent years, Bayesian nonparametric inference, both theoretical and computational, has witnessed considerable advances. However, these advances have not received a full critical and comparative analysis of their scope, impact and limitations in statistical modelling; many aspects of the theory and methods remain a mystery to practitioners and many open questions remain. In this paper, we discuss and illustrate the rich modelling and analytic possibilities that are available to the statistician within the Bayesian nonparametric and/or semiparametric framework.},
	Author = {Stephen G. Walker and Paul Damien and PuruShottam W. Laud and Adrian F. M. Smith},
	Doi = {10.1111/1467-9868.00190},
	Eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00190},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Keywords = {Dirichlet process, Exchangeability, L{\'e}vy process, Neutral to the right process, P{\'o}lya tree, Survival model},
	Number = {3},
	Pages = {485-527},
	Title = {Bayesian Nonparametric Inference for Random Distributions and Related Functions},
	Url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00190},
	Volume = {61},
	Bdsk-Url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00190},
	Bdsk-Url-2 = {https://doi.org/10.1111/1467-9868.00190}}

@article{yamato1984,
	Author = {Yamato, Hajime},
	Doi = {10.1214/aop/1176993389},
	Fjournal = {The Annals of Probability},
	Journal = {Ann. Probab.},
	Month = {02},
	Number = {1},
	Pages = {262--267},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Characteristic Functions of Means of Distributions Chosen from a Dirichlet Process},
	Url = {https://doi.org/10.1214/aop/1176993389},
	Volume = {12},
	Year = {1984},
	Bdsk-Url-1 = {https://doi.org/10.1214/aop/1176993389}}

@incollection{Sethuraman1982,
	Abstract = {Publisher Summary The form of the Bayes estimate of the population mean with respect to a Dirichlet prior with parameter a has given rise to the interpretation that a(X) is the prior sample size. Furthermore, if a(X) is made to tend to zero, then the Bayes estimate mathematically converges to the classical estimator, that is, the sample mean. This has further given rise to the general feeling that allowing a(X) to become small not only makes the prior sample size small but also that it corresponds to no prior information. By investigating the limits of prior distributions as the parameter a tends to various values, it is misleading to think of a(X) as the prior sample size and the smallness of a(X) as no prior information. In fact, very small values of a(X) actually mean that the prior has a lot of information concerning the unknown true distribution and is of a form that would be generally unacceptable to a statistician. },
	Author = {Jayaram Sethuraman and Ram C. Tiwari},
	Booktitle = {Statistical Decision Theory and Related Topics \{III\}},
	Doi = {https://doi.org/10.1016/B978-0-12-307502-4.50023-4},
	Editor = {Gupta, Shanti S. and , and Berger, James O.},
	Isbn = {978-0-12-307502-4},
	Pages = {305 - 315},
	Publisher = {Academic Press},
	Title = {\{CONVERGENCE\} \{OF\} \{DIRICHLET\} \{MEASURES\} \{AND\} \{THE\} \{INTERPRETATION\} \{OF\} \{THEIR\} \{PARAMETER1\}},
	Url = {https://www.sciencedirect.com/science/article/pii/B9780123075024500234},
	Year = {1982},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/B9780123075024500234},
	Bdsk-Url-2 = {https://doi.org/10.1016/B978-0-12-307502-4.50023-4}}

@article{sethuraman1994,
	Abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
	Author = {Jayaram Sethuraman},
	Issn = {10170405, 19968507},
	Journal = {Statistica Sinica},
	Number = {2},
	Pages = {639--650},
	Publisher = {Institute of Statistical Science, Academia Sinica},
	Title = {A CONSTRUCTIVE DEFINITION OF DIRICHLET PRIORS},
	Url = {http://www.jstor.org/stable/24305538},
	Volume = {4},
	Year = {1994},
	Bdsk-Url-1 = {http://www.jstor.org/stable/24305538}}

@article{blackwell1973,
	Author = {Blackwell, David and MacQueen, James B.},
	Doi = {10.1214/aos/1176342372},
	Fjournal = {The Annals of Statistics},
	Journal = {Ann. Statist.},
	Month = {03},
	Number = {2},
	Pages = {353--355},
	Publisher = {The Institute of Mathematical Statistics},
	Title = {Ferguson Distributions Via Polya Urn Schemes},
	Url = {https://doi.org/10.1214/aos/1176342372},
	Volume = {1},
	Year = {1973},
	Bdsk-Url-1 = {https://doi.org/10.1214/aos/1176342372}}

@inproceedings{Beal02,
	Author = {Matthew J. Beal and Zoubin Ghahramani and Carl E. Rasmussen},
	Booktitle = {Machine Learning},
	Pages = {29--245},
	Publisher = {MIT Press},
	Title = {The Infinite Hidden Markov Model},
	Year = {2002}}

@article{Teh06,
	Author = {Yee Whye Teh and Michael I Jordan and Matthew J Beal and David M Blei},
	Doi = {10.1198/016214506000000302},
	Eprint = {https://doi.org/10.1198/016214506000000302},
	Journal = {Journal of the American Statistical Association},
	Number = {476},
	Pages = {1566-1581},
	Publisher = {Taylor & Francis},
	Title = {Hierarchical Dirichlet Processes},
	Url = {https://doi.org/10.1198/016214506000000302},
	Volume = {101},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1198/016214506000000302}}

@inproceedings{Fox2008,
	Acmid = {1390196},
	Address = {New York, NY, USA},
	Author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
	Booktitle = {Proceedings of the 25th International Conference on Machine Learning},
	Doi = {10.1145/1390156.1390196},
	Isbn = {978-1-60558-205-4},
	Location = {Helsinki, Finland},
	Numpages = {8},
	Pages = {312--319},
	Publisher = {ACM},
	Series = {ICML '08},
	Title = {An HDP-HMM for Systems with State Persistence},
	Url = {http://doi.acm.org/10.1145/1390156.1390196},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1390156.1390196},
	Bdsk-Url-2 = {https://doi.org/10.1145/1390156.1390196}}

@inproceedings{Gael2008,
	Author = {Jurgen Van Gael and Yunus Saatci and Yee Whye Teh and Zoubin Ghahramani},
	Booktitle = {ICML},
	Title = {Beam sampling for the infinite hidden Markov model},
	Year = {2008}}

@inproceedings{Hughes2015,
	Acmid = {2969373},
	Address = {Cambridge, MA, USA},
	Author = {Hughes, Michael C. and Stephenson, William and Sudderth, Erik B.},
	Booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
	Location = {Montreal, Canada},
	Numpages = {9},
	Pages = {1198--1206},
	Publisher = {MIT Press},
	Series = {NIPS'15},
	Title = {Scalable Adaptation of State Complexity for Nonparametric Hidden Markov Models},
	Url = {http://dl.acm.org/citation.cfm?id=2969239.2969373},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2969239.2969373}}

@inproceedings{Johnson2014,
	Acmid = {3045099},
	Author = {Johnson, Matthew James and Willsky, Alan S.},
	Booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
	Location = {Beijing, China},
	Pages = {II-1854--II-1862},
	Publisher = {JMLR.org},
	Series = {ICML'14},
	Title = {Stochastic Variational Inference for Bayesian Time Series Models},
	Url = {http://dl.acm.org/citation.cfm?id=3044805.3045099},
	Year = {2014},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3044805.3045099}}

@incollection{Bryant2012,
	Author = {Michael Bryant and Erik B. Sudderth},
	Booktitle = {Advances in Neural Information Processing Systems 25},
	Editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	Pages = {2699--2707},
	Publisher = {Curran Associates, Inc.},
	Title = {Truly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes},
	Url = {http://papers.nips.cc/paper/4606-truly-nonparametric-online-variational-inference-for-hierarchical-dirichlet-processes.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/4606-truly-nonparametric-online-variational-inference-for-hierarchical-dirichlet-processes.pdf}}

@inproceedings{Liang2007TheIP,
	Author = {Percy Liang and Slav Petrov and Michael I. Jordan and Dan Klein},
	Booktitle = {EMNLP-CoNLL},
	Title = {The Infinite PCFG Using Hierarchical Dirichlet Processes},
	Year = {2007}}

@article{Steven2002,
	Abstract = {Markov chain Monte Carlo (MCMC) sampling strategies can be used to simulate hidden Markov model (HMM) parameters from their posterior distribution given observed data. Some MCMC methods used in practice (for computing likelihood, conditional probabilities of hidden states, and the most likely sequence of states) can be improved by incorporating established recursive algorithms. The most important of these is a set of forward-backward recursions calculating conditional distributions of the hidden states given observed data and model parameters. I show how to use the recursive algorithms in an MCMC context and demonstrate mathematical and empirical results showing a Gibbs sampler using the forward-backward recursions mixes more rapidly than another sampler often used for HMMs. I introduce an augmented variables technique for obtaining unique state labels in HMMs and finite mixture models. I show how recursive computing allows the statistically efficient use of MCMC output when estimating the hidden states. I directly calculate the posterior distribution of the hidden chain's state-space size by MCMC, circumventing asymptotic arguments underlying the Bayesian information criterion, which is shown to be inappropriate for a frequently analyzed dataset in the HMM literature. The use of log-likelihood for assessing MCMC convergence is illustrated, and posterior predictive checks are used to investigate application specific questions of model adequacy.},
	Author = {Steven L. Scott},
	Issn = {01621459},
	Journal = {Journal of the American Statistical Association},
	Number = {457},
	Pages = {337--351},
	Publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	Title = {Bayesian Methods for Hidden Markov Models: Recursive Computing in the 21st Century},
	Url = {http://www.jstor.org/stable/3085787},
	Volume = {97},
	Year = {2002},
	Bdsk-Url-1 = {http://www.jstor.org/stable/3085787}}

@article{Amy2010,
	Abstract = { Homogeneous multi-state models of disease progression have been widely used for designing and evaluating cancer screening programs. However, in screening for premalignant conditions of the cervix or large bowel, it is unlikely that all premalignant lesions have the same underlying propensity for progression. Incorporating frailty into multi-state models raises practical difficulties as it precludes the derivation of finite transition probabilities by matrix solution of the Kolmogorov equations. We address this problem by formulating a heterogeneous process as a series of homogeneous processes linked by transitions which are subject to heterogeneity (frailty). Continuous frailty and discrete mover---stayer models were developed. We applied these to the example of progression of adenoma to colorectal cancer in a three-state model and to a five-state model including consideration of adenoma size. Results were compared with those of purely homogeneous models in a previous study in terms of cumulative risk of malignant transformation from adenoma to invasive colorectal cancer. },
	Author = {Amy MF Yen and Tony HH Chen and Stephen W Duffy and Chih-Dao Chen},
	Doi = {10.1177/0962280209359862},
	Eprint = {https://doi.org/10.1177/0962280209359862},
	Journal = {Statistical Methods in Medical Research},
	Note = {PMID: 20488838},
	Number = {5},
	Pages = {529-546},
	Title = {Incorporating frailty in a multi-state model: application to disease natural history modelling of adenoma-carcinoma in the large bowel},
	Url = {https://doi.org/10.1177/0962280209359862},
	Volume = {19},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1177/0962280209359862}}

@article{Wei_2002,
	Author = {Wei Wei and Bing Wang and Don Towsley},
	Doi = {https://doi.org/10.1016/S0166-5316(02)00122-0},
	Issn = {0166-5316},
	Journal = {Performance Evaluation},
	Keywords = {Continuous-time hidden Markov model, EM algorithm, Performance evaluation, Network simulation},
	Note = {Performance 2002},
	Number = {1},
	Pages = {129 - 146},
	Title = {Continuous-time hidden Markov models for network performance evaluation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166531602001220},
	Volume = {49},
	Year = {2002},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0166531602001220},
	Bdsk-Url-2 = {https://doi.org/10.1016/S0166-5316(02)00122-0}}

@article{Bureau_2003,
	Abstract = {Abstract Disease progression in prospective clinical and epidemiological studies is often conceptualized in terms of transitions between disease states. Analysis of data from such studies can be complicated by a number of factors, including the presence of individuals in various prevalent disease states and with unknown prior disease history, interval censored observations of state transitions and misclassified measurements of disease states. We present an approach where the disease states are modelled as the hidden states of a continuous time hidden Markov model using the imperfect measurements of the disease state as observations. Covariate effects on transitions between disease states are incorporated using a generalized regression framework. Parameter estimation and inference are based on maximum likelihood methods and rely on an EM algorithm. In addition, techniques for model assessment are proposed. Applications to two binary disease outcomes are presented: the oral lesion hairy leukoplakia in a cohort of HIV infected men and cervical human papillomavirus (HPV) infection in a cohort of young women. Estimated transition rates and misclassification probabilities for the hairy leukoplakia data agree well with clinical observations on the persistence and diagnosis of this lesion, lending credibility to the interpretation of hidden states as representing the actual disease states. By contrast, interpretation of the results for the HPV data are more problematic, illustrating that successful application of the hidden Markov model may be highly dependent on the degree to which the assumptions of the model are satisfied. Copyright {\copyright} 2003 John Wiley \& Sons, Ltd.},
	Author = {Bureau, Alexandre and Shiboski, Stephen and Hughes, James P.},
	Doi = {10.1002/sim.1270},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1270},
	Journal = {Statistics in Medicine},
	Keywords = {hidden Markov models, longitudinal data, infectious diseases, misclassification},
	Number = {3},
	Pages = {441-462},
	Title = {Applications of continuous time hidden Markov models to the study of misclassified disease outcomes},
	Url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1270},
	Volume = {22},
	Year = {2003},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1270},
	Bdsk-Url-2 = {https://doi.org/10.1002/sim.1270}}

@article{Lu_2017,
	Author = {Shaochuan Lu},
	Doi = {10.1080/02664763.2016.1161736},
	Eprint = {https://doi.org/10.1080/02664763.2016.1161736},
	Journal = {Journal of Applied Statistics},
	Number = {1},
	Pages = {71-88},
	Publisher = {Taylor & Francis},
	Title = {A continuous-time HMM approach to modeling the magnitude-frequency distribution of earthquakes},
	Url = {https://doi.org/10.1080/02664763.2016.1161736},
	Volume = {44},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1080/02664763.2016.1161736}}

@article{Krishnamurthy_2016,
	title = "Filterbased stochastic volatility in continuous-time hidden Markov models",
	journal = "Econometrics and Statistics",
	volume = "6",
	pages = "1 - 21",
	year = "2018",
	note = "STATISTICS OF EXTREMES AND APPLICATIONS",
	issn = "2452-3062",
	doi = "https://doi.org/10.1016/j.ecosta.2016.10.007",
	url = "http://www.sciencedirect.com/science/article/pii/S2452306216300144",
	author = "Vikram Krishnamurthy and Elisabeth Leoff and Jörn Sass",
	keywords = "Markov switching model, Non-constant volatility, Stylized facts, Portfolio optimization, Social learning",
	abstract = "Regime-switching models, in particular Hidden Markov Models (HMMs) where the switching is driven by an unobservable Markov chain, are widely-used in financial applications, due to their tractability and good econometric properties. In continuous time, properties of HMMs with constant and of HMMs with switching volatility can be quite different. To have a realistic model with unobservable Markov chain in continuous time and good econometric properties, a regime-switching model where the volatility depends on the filter for the underlying chain is introduced and the filtering equations are stated. Such models are motivated by agent based social learning models in economics. An approximation result for a fixed information filtration is proved and further motivation is provided by considering social learning arguments. The relation to the switching volatility model is analyzed in detail and a convergence result for the discretized model is given. Econometric properties are illustrated by numerical simulations."}

@incollection{Liu_2015,
	title = {Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression},
	author = {Liu, Yu-Ying and Li, Shuang and Li, Fuxin and Song, Le and Rehg, James M},
	booktitle = {Advances in Neural Information Processing Systems 28},
	editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	pages = {3600--3608},
	year = {2015},
	publisher = {Curran Associates, Inc.}
}

@article{Zeifman_1994,
	Author = {A.I. Zeifman and Dean L. Isaacson},
	Doi = {https://doi.org/10.1016/0304-4149(94)90123-6},
	Issn = {0304-4149},
	Journal = {Stochastic Processes and their Applications},
	Keywords = {nonhomogeneous, continuous-time Markov chain, strong ergodicity, weak ergodicity, forward Kolmogorov system, differential equation in the space},
	Number = {2},
	Pages = {263 - 273},
	Title = {On strong ergodicity for nonhomogeneous continuous-time Markov chains},
	Url = {http://www.sciencedirect.com/science/article/pii/0304414994901236},
	Volume = {50},
	Year = {1994},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0304414994901236},
	Bdsk-Url-2 = {https://doi.org/10.1016/0304-4149(94)90123-6}}

@article{Metzner_2007,
	Author = {Metzner, Philipp and Horenko, Illia and Sch{\"u}tte, Christof},
	Journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},
	Month = {01},
	Pages = {066702},
	Title = {Generator estimation of Markov jump processes based on incomplete observations nonequidistant in time},
	Volume = {76},
	Year = {2008}}

@article{Mogens_2005,
	Abstract = {Likelihood inference for discretely observed Markov jump processes with finite state space is investigated. The existence and uniqueness of the maximum likelihood estimator of the intensity matrix are investigated. This topic is closely related to the imbedding problem for Markov chains. It is demonstrated that the maximum likelihood estimator can be found either by the EM algorithm or by a Markov chain Monte Carlo procedure. When the maximum likelihood estimator does not exist, an estimator can be obtained by using a penalized likelihood function or by the Markov chain Monte Carlo procedure with a suitable prior. The methodology and its implementation are illustrated by examples and simulation studies.},
	Author = {Mogens Bladt and Michael S{\o}rensen},
	Issn = {13697412, 14679868},
	Journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	Number = {3},
	Pages = {395--410},
	Publisher = {[Royal Statistical Society, Wiley]},
	Title = {Statistical Inference for Discretely Observed Markov Jump Processes},
	Url = {http://www.jstor.org/stable/3647667},
	Volume = {67},
	Year = {2005},
	Bdsk-Url-1 = {http://www.jstor.org/stable/3647667}}

@article{Loan_1978,
	Author = {C. Van Loan},
	Doi = {10.1109/TAC.1978.1101743},
	Issn = {0018-9286},
	Journal = {IEEE Transactions on Automatic Control},
	Keywords = {Linear systems, time-invariant discrete-time;Matrix functions;Numerical integration;Optimal regulators;Differential equations;Finite wordlength effects;Algorithms;Integral equations;Regulators;Computer science},
	Month = {June},
	Number = {3},
	Pages = {395-404},
	Title = {Computing integrals involving the matrix exponential},
	Volume = {23},
	Year = {1978},
	Bdsk-Url-1 = {https://doi.org/10.1109/TAC.1978.1101743}}

@article{Hobolth_2011,
	Author = {Hobolth, Asger and Jensen, Jens Ledet},
	Doi = {10.1239/jap/1324046009},
	Fjournal = {Journal of Applied Probability},
	Journal = {J. Appl. Probab.},
	Month = {12},
	Number = {4},
	Pages = {911--924},
	Publisher = {Applied Probability Trust},
	Title = {Summary statistics for endpoint-conditioned continuous-time Markov chains},
	Url = {https://doi.org/10.1239/jap/1324046009},
	Volume = {48},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1239/jap/1324046009}}

@article{Kitagawa_1987,
	Abstract = {A non-Gaussian state-space approach to the modeling of nonstationary time series is shown. The model is expressed in state-space form, where the system noise and the observational noise are not necessarily Gaussian. Recursive formulas of prediction, filtering, and smoothing for the state estimation and identification of the non-Gaussian state-space model are given. Also given is a numerical method based on piecewise linear approximation to the density functions for realizing these formulas. Significant merits of non-Gaussian modeling and the wide range of applicability of the method are illustrated by some numerical examples. A typical application of this non-Gaussian modeling is the smoothing of a time series that has mean value function with both abrupt and gradual changes. Simple Gaussian state-space modeling is not adequate for this situation. Here the model with small system noise variance cannot detect jump, whereas the one with large system noise variance yields unfavorable wiggle. To work out this problem within the ordinary linear Gaussian model framework, sophisticated treatment of outliers is required. But by the use of an appropriate non-Gaussian model for system noise, it is possible to reproduce both abrupt and gradual change of the mean without any special treatment. Nonstandard observations such as the ones distributed as non-Gaussian distribution can be easily treated by the direct modeling of an observational scheme. Smoothing of a transformed series such as a log periodogram can be treated by this method. Outliers in the observations can be treated as well by using heavy-tailed distribution for observational noise density. The algorithms herein can be easily extended to a wider class of models. As an example, the smoothing of nonhomogeneous binomial mean function is shown, where the observation is distributed according to a discrete random variable. Extension to a nonlinear system is also straightforward.},
	Author = {Genshiro Kitagawa},
	Issn = {01621459},
	Journal = {Journal of the American Statistical Association},
	Number = {400},
	Pages = {1032--1041},
	Publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
	Title = {Non-Gaussian State-Space Modeling of Nonstationary Time Series},
	Url = {http://www.jstor.org/stable/2289375},
	Volume = {82},
	Year = {1987},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2289375}}

@article{Baydin_2018,
	Author = {Baydin, Atilim Gunes and Pearlmutter, Barak and Andreyevich Radul, Alexey and Siskind, Jeffrey},
	Date-Modified = {2019-02-01 10:40:43 -0800},
	Journal = {Journal of Machine Learning Research},
	Month = {04},
	Pages = {1-43},
	Title = {Automatic differentiation in machine learning: A survey},
	Volume = {18},
	Year = {2018}}



@article{Larsen_2009,
	author = {Larsen, I. K. and Smastuen, M. and Johannesen, T. B. and Langmark, F. and Parkin, D. M. and Bray, F. and Moller, B.},
	title = {Data quality at the Cancer Registry of Norway: an overview of comparability, completeness, validity and timeliness},
	journal = {Eur J Cancer},
	volume = {45},
	number = {7},
	pages = {1218-31},
	ISSN = {1879-0852 (Electronic)},
	url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Citation&list_uids=19091545},
	year = {2009},
	type = {Journal Article}
}
@article{Leinonen_2018,
	author = {Leinonen, M. K. and Hansen, S. A. and Skare, G. B. and Skaaret, I. B. and Silva, M. and Johannesen, T. B. and Nygard, M.},
	title = {Low proportion of unreported cervical treatments in the cancer registry of Norway between 1998 and 2013},
	journal = {Acta Oncol},
	volume = {57},
	number = {12},
	pages = {1663-1670},
	ISSN = {1651-226X (Electronic)
	0284-186X (Linking)},
	DOI = {10.1080/0284186X.2018.1497296},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/30169991},
	year = {2018},
	type = {Journal Article}
}


@article{Ursin_2017,
	author = {Ursin, G. and Sen, S. and Mottu, J. M. and Nygard, M.},
	title = {Protecting Privacy in Large Datasets-First We Assess the Risk; Then We Fuzzy the Data},
	journal = {Cancer Epidemiol Biomarkers Prev},
	ISSN = {1538-7755 (Electronic)
	1055-9965 (Linking)},
	DOI = {10.1158/1055-9965.EPI-17-0172},
	url = {https://www.ncbi.nlm.nih.gov/pubmed/28754793},
	year = {2017},
	type = {Journal Article},
	volume = {26},
	number = {8},
	pages = {1219-1224}
}

@phdthesis{Maclaurin_2016,
	title    = {Modeling, Inference and Optimization with Compposable Differentiable Procedures},
	school   = {Harvard University},
	author   = {Maclaurin, Dougal},
	year     = {2016},
	type     = {{PhD} dissertation},
}
